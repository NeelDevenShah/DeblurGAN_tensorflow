{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1>Whole Implementation of DeBlurGan using Tensorflow and Keras</h1>"]},{"cell_type":"markdown","metadata":{},"source":["organize_gopro_dataset.py"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"c769b41c-4006-4ebd-8714-50b01ae8f75e","_uuid":"089c4f12-082c-4fd7-addb-090fc3c72efa","collapsed":false,"execution":{"iopub.execute_input":"2023-07-15T17:10:19.385388Z","iopub.status.busy":"2023-07-15T17:10:19.384970Z","iopub.status.idle":"2023-07-15T17:12:11.852529Z","shell.execute_reply":"2023-07-15T17:12:11.851561Z","shell.execute_reply.started":"2023-07-15T17:10:19.385335Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["dir:   0%|          | 0/2 [00:00<?, ?it/s]\n","image_folders:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n","image_folders:   9%|▉         | 1/11 [00:04<00:45,  4.60s/it]\u001b[A\n","image_folders:  18%|█▊        | 2/11 [00:07<00:32,  3.57s/it]\u001b[A\n","image_folders:  27%|██▋       | 3/11 [00:10<00:28,  3.53s/it]\u001b[A\n","image_folders:  36%|███▋      | 4/11 [00:14<00:24,  3.57s/it]\u001b[A\n","image_folders:  45%|████▌     | 5/11 [00:18<00:21,  3.52s/it]\u001b[A\n","image_folders:  55%|█████▍    | 6/11 [00:21<00:16,  3.36s/it]\u001b[A\n","image_folders:  64%|██████▎   | 7/11 [00:24<00:13,  3.45s/it]\u001b[A\n","image_folders:  73%|███████▎  | 8/11 [00:27<00:10,  3.40s/it]\u001b[A\n","image_folders:  82%|████████▏ | 9/11 [00:33<00:07,  3.92s/it]\u001b[A\n","image_folders:  91%|█████████ | 10/11 [00:37<00:04,  4.03s/it]\u001b[A\n","image_folders: 100%|██████████| 11/11 [00:40<00:00,  3.71s/it]\u001b[A\n","dir:  50%|█████     | 1/2 [00:40<00:40, 40.83s/it]\n","image_folders:   0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n","image_folders:   5%|▍         | 1/22 [00:01<00:40,  1.92s/it]\u001b[A\n","image_folders:   9%|▉         | 2/22 [00:04<00:49,  2.46s/it]\u001b[A\n","image_folders:  14%|█▎        | 3/22 [00:08<00:57,  3.01s/it]\u001b[A\n","image_folders:  18%|█▊        | 4/22 [00:09<00:43,  2.44s/it]\u001b[A\n","image_folders:  23%|██▎       | 5/22 [00:13<00:47,  2.81s/it]\u001b[A\n","image_folders:  27%|██▋       | 6/22 [00:16<00:47,  2.95s/it]\u001b[A\n","image_folders:  32%|███▏      | 7/22 [00:19<00:42,  2.85s/it]\u001b[A\n","image_folders:  36%|███▋      | 8/22 [00:21<00:38,  2.77s/it]\u001b[A\n","image_folders:  41%|████      | 9/22 [00:25<00:40,  3.08s/it]\u001b[A\n","image_folders:  45%|████▌     | 10/22 [00:29<00:40,  3.34s/it]\u001b[A\n","image_folders:  50%|█████     | 11/22 [00:33<00:37,  3.44s/it]\u001b[A\n","image_folders:  55%|█████▍    | 12/22 [00:37<00:38,  3.82s/it]\u001b[A\n","image_folders:  59%|█████▉    | 13/22 [00:40<00:31,  3.50s/it]\u001b[A\n","image_folders:  64%|██████▎   | 14/22 [00:42<00:25,  3.13s/it]\u001b[A\n","image_folders:  68%|██████▊   | 15/22 [00:46<00:23,  3.39s/it]\u001b[A\n","image_folders:  73%|███████▎  | 16/22 [00:50<00:19,  3.31s/it]\u001b[A\n","image_folders:  77%|███████▋  | 17/22 [00:53<00:17,  3.44s/it]\u001b[A\n","image_folders:  82%|████████▏ | 18/22 [00:57<00:14,  3.51s/it]\u001b[A\n","image_folders:  86%|████████▋ | 19/22 [01:01<00:10,  3.60s/it]\u001b[A\n","image_folders:  91%|█████████ | 20/22 [01:05<00:07,  3.64s/it]\u001b[A\n","image_folders:  95%|█████████▌| 21/22 [01:08<00:03,  3.58s/it]\u001b[A\n","image_folders: 100%|██████████| 22/22 [01:11<00:00,  3.26s/it]\u001b[A\n","dir: 100%|██████████| 2/2 [01:52<00:00, 56.22s/it]\n"]}],"source":["import os\n","from shutil import copyfile\n","\n","import click\n","import tqdm\n","\n","def reorganize_gopro_files(dir_in, dir_out):\n","    if not os.path.exists(dir_out):\n","        os.makedirs(dir_out)\n","\n","    for folder_train_test in tqdm.tqdm(os.listdir(dir_in), desc='dir'):\n","        output_directory = os.path.join(dir_out, folder_train_test)\n","        output_directory_A = os.path.join(output_directory, 'A')\n","        output_directory_B = os.path.join(output_directory, 'B')\n","        if not os.path.exists(output_directory):\n","            os.makedirs(output_directory)\n","        if not os.path.exists(output_directory_A):\n","            os.makedirs(output_directory_A)\n","        if not os.path.exists(output_directory_B):\n","            os.makedirs(output_directory_B)\n","\n","        current_folder_path = os.path.join(dir_in, folder_train_test)\n","        for image_folder in tqdm.tqdm(os.listdir(current_folder_path), desc='image_folders'):\n","\n","            current_sub_folder_path = os.path.join(\n","                current_folder_path, image_folder)\n","\n","            for image_blurred in os.listdir(os.path.join(current_sub_folder_path, 'blur')):\n","                current_image_blurred_path = os.path.join(\n","                    current_sub_folder_path, 'blur', image_blurred)\n","                output_image_blurred_path = os.path.join(\n","                    output_directory_A, image_folder + \"_\" + image_blurred)\n","                copyfile(current_image_blurred_path, output_image_blurred_path)\n","\n","            for image_sharp in os.listdir(os.path.join(current_sub_folder_path, 'sharp')):\n","                current_image_sharp_path = os.path.join(\n","                    current_sub_folder_path, 'sharp', image_sharp)\n","                output_image_sharp_path = os.path.join(\n","                    output_directory_B, image_folder + \"_\" + image_sharp)\n","                copyfile(current_image_sharp_path, output_image_sharp_path)\n","\n","\n","if __name__ == \"__main__\":\n","    reorganize_gopro_files('/kaggle/input/gopro-data', '/kaggle/working/')\n"]},{"cell_type":"markdown","metadata":{},"source":["layer_utils.py"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-15T17:12:11.855445Z","iopub.status.busy":"2023-07-15T17:12:11.854840Z","iopub.status.idle":"2023-07-15T17:12:11.968268Z","shell.execute_reply":"2023-07-15T17:12:11.967500Z","shell.execute_reply.started":"2023-07-15T17:12:11.855410Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-15 23:37:57.873118: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-07-15 23:37:57.916071: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2023-07-15 23:37:57.916087: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n","                                                                 \n"," reflection_padding2d (Refle  (None, 262, 262, 3)      0         \n"," ctionPadding2D)                                                 \n","                                                                 \n","=================================================================\n","Total params: 0\n","Trainable params: 0\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-15 23:37:59.946300: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2023-07-15 23:37:59.946317: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2023-07-15 23:37:59.946328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pop-os): /proc/driver/nvidia/version does not exist\n","2023-07-15 23:37:59.946955: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import tensorflow as tf\n","\n","from keras.models import Model\n","from keras.layers import InputSpec, Layer\n","from keras.layers import Input, Conv2D, Activation, BatchNormalization\n","from keras.layers import Add\n","from keras.utils import conv_utils\n","from keras.layers.core import Dropout\n","\n","\n","def res_block(input, filters, kernel_size=(3, 3), strides=(1, 1), use_dropout=False):\n","    \"\"\"\n","    Instanciate a Keras Resnet Block using sequential API.\n","\n","    :param input: Input tensor\n","    :param filters: Number of filters to use\n","    :param kernel_size: Shape of the kernel for the convolution\n","    :param strides: Shape of the strides for the convolution\n","    :param use_dropout: Boolean value to determine the use of dropout\n","    :return: Keras Model\n","    \"\"\"\n","    x = ReflectionPadding2D((1, 1))(input)\n","    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    if use_dropout:\n","        x = Dropout(0.5)(x)\n","\n","    x = ReflectionPadding2D((1, 1))(x)\n","    x = Conv2D(filters=filters,\n","               kernel_size=kernel_size,\n","               strides=strides,)(x)\n","    x = BatchNormalization()(x)\n","\n","    merged = Add()([input, x])\n","    return merged\n","\n","\n","def spatial_reflection_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n","    \"\"\"\n","    Pad the 2nd and 3rd dimensions of a 4D tensor.\n","\n","    :param x: Input tensor\n","    :param padding: Shape of padding to use\n","    :param data_format: Tensorflow vs Theano convention ('channels_last', 'channels_first')\n","    :return: Tensorflow tensor\n","    \"\"\"\n","    assert len(padding) == 2\n","    assert len(padding[0]) == 2\n","    assert len(padding[1]) == 2\n","    if data_format is None:\n","        data_format = image_data_format()\n","    if data_format not in {'channels_first', 'channels_last'}:\n","        raise ValueError('Unknown data_format ' + str(data_format))\n","\n","    if data_format == 'channels_first':\n","        pattern = [[0, 0],\n","                   [0, 0],\n","                   list(padding[0]),\n","                   list(padding[1])]\n","    else:\n","        pattern = [[0, 0],\n","                   list(padding[0]), list(padding[1]),\n","                   [0, 0]]\n","    return tf.pad(x, pattern, \"REFLECT\")\n","\n","\n","class ReflectionPadding2D(Layer):\n","    \"\"\"Reflection-padding layer for 2D input (e.g. picture).\n","    This layer can add rows and columns or zeros\n","    at the top, bottom, left and right side of an image tensor.\n","    # Arguments\n","        padding: int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n","            - If int: the same symmetric padding is applied to width and height.\n","            - If tuple of 2 ints:\n","                interpreted as two different\n","                symmetric padding values for height and width:\n","                `(symmetric_height_pad, symmetric_width_pad)`.\n","            - If tuple of 2 tuples of 2 ints:\n","                interpreted as\n","                `((top_pad, bottom_pad), (left_pad, right_pad))`\n","        data_format: A string,\n","            one of `channels_last` (default) or `channels_first`.\n","            The ordering of the dimensions in the inputs.\n","            `channels_last` corresponds to inputs with shape\n","            `(batch, height, width, channels)` while `channels_first`\n","            corresponds to inputs with shape\n","            `(batch, channels, height, width)`.\n","            It defaults to the `image_data_format` value found in your\n","            Keras config file at `~/.keras/keras.json`.\n","            If you never set it, then it will be \"channels_last\".\n","    # Input shape\n","        4D tensor with shape:\n","        - If `data_format` is `\"channels_last\"`:\n","            `(batch, rows, cols, channels)`\n","        - If `data_format` is `\"channels_first\"`:\n","            `(batch, channels, rows, cols)`\n","    # Output shape\n","        4D tensor with shape:\n","        - If `data_format` is `\"channels_last\"`:\n","            `(batch, padded_rows, padded_cols, channels)`\n","        - If `data_format` is `\"channels_first\"`:\n","            `(batch, channels, padded_rows, padded_cols)`\n","    \"\"\"\n","\n","    def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n","        super(ReflectionPadding2D, self).__init__(**kwargs)\n","        self.data_format = conv_utils.normalize_data_format(data_format)\n","\n","        if isinstance(padding, int):\n","            self.padding = ((padding, padding), (padding, padding))\n","\n","        elif hasattr(padding, '__len__'):\n","\n","            if len(padding) != 2:\n","                raise ValueError(\n","                    '`padding` should have two elements. ' 'Found: ' + str(padding))\n","            height_padding = conv_utils.normalize_tuple(padding[0], 2,\n","                                                        '1st entry of padding')\n","            width_padding = conv_utils.normalize_tuple(padding[1], 2,\n","                                                       '2nd entry of padding')\n","            self.padding = (height_padding, width_padding)\n","        else:\n","            raise ValueError('`padding` should be either an int, '\n","                             'a tuple of 2 ints '\n","                             '(symmetric_height_pad, symmetric_width_pad), '\n","                             'or a tuple of 2 tuples of 2 ints '\n","                             '((top_pad, bottom_pad), (left_pad, right_pad)). '\n","                             'Found: ' + str(padding))\n","        self.input_spec = InputSpec(ndim=4)\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.data_format == 'channels_first':\n","            if input_shape[2] is not None:\n","                rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n","            else:\n","                rows = None\n","\n","            if input_shape[3] is not None:\n","                cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n","            else:\n","                cols = None\n","\n","            return (input_shape[0], input_shape[1], rows, cols)\n","\n","        elif self.data_format == 'channels_last':\n","            if input_shape[1] is not None:\n","                rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n","            else:\n","                rows = None\n","\n","            if input_shape[2] is not None:\n","                cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n","            else:\n","                cols = None\n","\n","            return (input_shape[0], rows, cols, input_shape[3])\n","\n","    def call(self, inputs):\n","        return spatial_reflection_2d_padding(inputs, padding=self.padding, data_format=self.data_format)\n","\n","    def get_config(self):\n","        config = {'padding': self.padding, 'data_format': self.data_format}\n","        base_config = super(ReflectionPadding2D, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","if __name__ == \"__main__\":\n","    input = Input(shape=(256, 256, 3))\n","    x = ReflectionPadding2D(3)(input)\n","    model = Model(input, x)\n","    model.summary()\n"]},{"cell_type":"markdown","metadata":{},"source":["losses.py"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-15T17:22:38.930475Z","iopub.status.busy":"2023-07-15T17:22:38.929502Z","iopub.status.idle":"2023-07-15T17:22:39.253858Z","shell.execute_reply":"2023-07-15T17:22:39.252851Z","shell.execute_reply.started":"2023-07-15T17:22:38.930436Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 174s 3us/step\n"]}],"source":["import keras.backend as k\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Model\n","import numpy as np\n","\n","# Note the image_shape must be multiple of patch_shape\n","image_shape = (256, 256, 3)\n","\n","\n","def l1_loss(y_true, y_pred):\n","    return k.mean(k.abs(y_pred - y_true))\n","\n","vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n","# vgg_variables = vgg.trainable_variables\n","# loss_model = Model(vgg_variables)\n","\n","loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n","loss_model.trainable = False\n","\n","@tf.function\n","def perceptual_loss(y_true, y_pred):\n","    features1 = loss_model(y_true)\n","    features2 = loss_model(y_pred)\n","    loss = tf.reduce_mean(tf.square(features1 - features2))\n","    return loss\n","\n","def perceptual_loss_100(y_true, y_pred):\n","    return 100 * perceptual_loss(y_true, y_pred)\n","\n","\n","def wasserstein_loss(y_true, y_pred):\n","    return k.mean(y_true*y_pred)\n","\n","\n","def gradient_penalty_loss(self, y_ture, y_pred, averaged_samples):\n","    gradients = k.gradients(y_pred, averaged_samples)[0]\n","    gradients_sqr = k.square(gradients)\n","    gradients_sqr_sum = k.sum(\n","        gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n","    gradient_l2_norm = k.sqrt(gradients_sqr_sum)\n","    gradient_penalty = k.square(1 - gradient_l2_norm)\n","    return k.mean(gradient_penalty)"]},{"cell_type":"markdown","metadata":{},"source":["model.py"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-15T17:12:13.955372Z","iopub.status.busy":"2023-07-15T17:12:13.955005Z","iopub.status.idle":"2023-07-15T17:12:16.928857Z","shell.execute_reply":"2023-07-15T17:12:16.927861Z","shell.execute_reply.started":"2023-07-15T17:12:13.955322Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"Generator\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 256, 256, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," reflection_padding2d_1 (Reflec  (None, 262, 262, 3)  0          ['input_3[0][0]']                \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 256, 256, 64  9472        ['reflection_padding2d_1[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['conv2d[0][0]']                 \n"," alization)                     )                                                                 \n","                                                                                                  \n"," activation (Activation)        (None, 256, 256, 64  0           ['batch_normalization[0][0]']    \n","                                )                                                                 \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 128, 128, 12  73856       ['activation[0][0]']             \n","                                8)                                                                \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 128, 128, 12  512        ['conv2d_1[0][0]']               \n"," rmalization)                   8)                                                                \n","                                                                                                  \n"," activation_1 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_1[0][0]']  \n","                                8)                                                                \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 64, 64, 256)  295168      ['activation_1[0][0]']           \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_2[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_2 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_2[0][0]']  \n","                                                                                                  \n"," reflection_padding2d_2 (Reflec  (None, 66, 66, 256)  0          ['activation_2[0][0]']           \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_2[0][0]'] \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_3[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_3 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," dropout (Dropout)              (None, 64, 64, 256)  0           ['activation_3[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_3 (Reflec  (None, 66, 66, 256)  0          ['dropout[0][0]']                \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_3[0][0]'] \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add (Add)                      (None, 64, 64, 256)  0           ['activation_2[0][0]',           \n","                                                                  'batch_normalization_4[0][0]']  \n","                                                                                                  \n"," reflection_padding2d_4 (Reflec  (None, 66, 66, 256)  0          ['add[0][0]']                    \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_4[0][0]'] \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_5[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 64, 64, 256)  0           ['activation_4[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_5 (Reflec  (None, 66, 66, 256)  0          ['dropout_1[0][0]']              \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_5[0][0]'] \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_6[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_1 (Add)                    (None, 64, 64, 256)  0           ['add[0][0]',                    \n","                                                                  'batch_normalization_6[0][0]']  \n","                                                                                                  \n"," reflection_padding2d_6 (Reflec  (None, 66, 66, 256)  0          ['add_1[0][0]']                  \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_6[0][0]'] \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_7[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_5 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 64, 64, 256)  0           ['activation_5[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_7 (Reflec  (None, 66, 66, 256)  0          ['dropout_2[0][0]']              \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_8 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_7[0][0]'] \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_8[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_2 (Add)                    (None, 64, 64, 256)  0           ['add_1[0][0]',                  \n","                                                                  'batch_normalization_8[0][0]']  \n","                                                                                                  \n"," reflection_padding2d_8 (Reflec  (None, 66, 66, 256)  0          ['add_2[0][0]']                  \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_9 (Conv2D)              (None, 64, 64, 256)  590080      ['reflection_padding2d_8[0][0]'] \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_9[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_6 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 64, 64, 256)  0           ['activation_6[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_9 (Reflec  (None, 66, 66, 256)  0          ['dropout_3[0][0]']              \n"," tionPadding2D)                                                                                   \n","                                                                                                  \n"," conv2d_10 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_9[0][0]'] \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_10[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_3 (Add)                    (None, 64, 64, 256)  0           ['add_2[0][0]',                  \n","                                                                  'batch_normalization_10[0][0]'] \n","                                                                                                  \n"," reflection_padding2d_10 (Refle  (None, 66, 66, 256)  0          ['add_3[0][0]']                  \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_11 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_10[0][0]']\n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_11[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_7 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_11[0][0]'] \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 64, 64, 256)  0           ['activation_7[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_11 (Refle  (None, 66, 66, 256)  0          ['dropout_4[0][0]']              \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_12 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_11[0][0]']\n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_12[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_4 (Add)                    (None, 64, 64, 256)  0           ['add_3[0][0]',                  \n","                                                                  'batch_normalization_12[0][0]'] \n","                                                                                                  \n"," reflection_padding2d_12 (Refle  (None, 66, 66, 256)  0          ['add_4[0][0]']                  \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_12[0][0]']\n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_13[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_8 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 64, 64, 256)  0           ['activation_8[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_13 (Refle  (None, 66, 66, 256)  0          ['dropout_5[0][0]']              \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_13[0][0]']\n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_14[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_5 (Add)                    (None, 64, 64, 256)  0           ['add_4[0][0]',                  \n","                                                                  'batch_normalization_14[0][0]'] \n","                                                                                                  \n"," reflection_padding2d_14 (Refle  (None, 66, 66, 256)  0          ['add_5[0][0]']                  \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_14[0][0]']\n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_15[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_9 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 64, 64, 256)  0           ['activation_9[0][0]']           \n","                                                                                                  \n"," reflection_padding2d_15 (Refle  (None, 66, 66, 256)  0          ['dropout_6[0][0]']              \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_16 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_15[0][0]']\n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_16[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_6 (Add)                    (None, 64, 64, 256)  0           ['add_5[0][0]',                  \n","                                                                  'batch_normalization_16[0][0]'] \n","                                                                                                  \n"," reflection_padding2d_16 (Refle  (None, 66, 66, 256)  0          ['add_6[0][0]']                  \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_17 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_16[0][0]']\n","                                                                                                  \n"," batch_normalization_17 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_17[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_10 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_17[0][0]'] \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 64, 64, 256)  0           ['activation_10[0][0]']          \n","                                                                                                  \n"," reflection_padding2d_17 (Refle  (None, 66, 66, 256)  0          ['dropout_7[0][0]']              \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_18 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_17[0][0]']\n","                                                                                                  \n"," batch_normalization_18 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_18[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_7 (Add)                    (None, 64, 64, 256)  0           ['add_6[0][0]',                  \n","                                                                  'batch_normalization_18[0][0]'] \n","                                                                                                  \n"," reflection_padding2d_18 (Refle  (None, 66, 66, 256)  0          ['add_7[0][0]']                  \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_19 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_18[0][0]']\n","                                                                                                  \n"," batch_normalization_19 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_19[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_11 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_19[0][0]'] \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, 64, 64, 256)  0           ['activation_11[0][0]']          \n","                                                                                                  \n"," reflection_padding2d_19 (Refle  (None, 66, 66, 256)  0          ['dropout_8[0][0]']              \n"," ctionPadding2D)                                                                                  \n","                                                                                                  \n"," conv2d_20 (Conv2D)             (None, 64, 64, 256)  590080      ['reflection_padding2d_19[0][0]']\n","                                                                                                  \n"," batch_normalization_20 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_20[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_8 (Add)                    (None, 64, 64, 256)  0           ['add_7[0][0]',                  \n","                                                                  'batch_normalization_20[0][0]'] \n","                                                                                                  \n"," up_sampling2d (UpSampling2D)   (None, 128, 128, 25  0           ['add_8[0][0]']                  \n","                                6)                                                                \n","                                                                                                  \n"," conv2d_21 (Conv2D)             (None, 128, 128, 12  295040      ['up_sampling2d[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," batch_normalization_21 (BatchN  (None, 128, 128, 12  512        ['conv2d_21[0][0]']              \n"," ormalization)                  8)                                                                \n","                                                                                                  \n"," activation_12 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_21[0][0]'] \n","                                8)                                                                \n","                                                                                                  \n"," up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 12  0          ['activation_12[0][0]']          \n","                                8)                                                                \n","                                                                                                  \n"," conv2d_22 (Conv2D)             (None, 256, 256, 64  73792       ['up_sampling2d_1[0][0]']        \n","                                )                                                                 \n","                                                                                                  \n"," batch_normalization_22 (BatchN  (None, 256, 256, 64  256        ['conv2d_22[0][0]']              \n"," ormalization)                  )                                                                 \n","                                                                                                  \n"," activation_13 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_22[0][0]'] \n","                                )                                                                 \n","                                                                                                  \n"," reflection_padding2d_20 (Refle  (None, 262, 262, 64  0          ['activation_13[0][0]']          \n"," ctionPadding2D)                )                                                                 \n","                                                                                                  \n"," conv2d_23 (Conv2D)             (None, 256, 256, 3)  9411        ['reflection_padding2d_20[0][0]']\n","                                                                                                  \n"," activation_14 (Activation)     (None, 256, 256, 3)  0           ['conv2d_23[0][0]']              \n","                                                                                                  \n"," add_9 (Add)                    (None, 256, 256, 3)  0           ['activation_14[0][0]',          \n","                                                                  'input_3[0][0]']                \n","                                                                                                  \n"," lambda (Lambda)                (None, 256, 256, 3)  0           ['add_9[0][0]']                  \n","                                                                                                  \n","==================================================================================================\n","Total params: 11,399,171\n","Trainable params: 11,388,675\n","Non-trainable params: 10,496\n","__________________________________________________________________________________________________\n","Model: \"Discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 128, 128, 64)      3136      \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 128, 128, 64)      0         \n","                                                                 \n"," conv2d_25 (Conv2D)          (None, 128, 128, 64)      65600     \n","                                                                 \n"," batch_normalization_23 (Bat  (None, 128, 128, 64)     256       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 128, 128, 64)      0         \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 128, 128, 128)     131200    \n","                                                                 \n"," batch_normalization_24 (Bat  (None, 128, 128, 128)    512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 128, 128, 128)     0         \n","                                                                 \n"," conv2d_27 (Conv2D)          (None, 128, 128, 256)     524544    \n","                                                                 \n"," batch_normalization_25 (Bat  (None, 128, 128, 256)    1024      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 128, 128, 256)     0         \n","                                                                 \n"," conv2d_28 (Conv2D)          (None, 128, 128, 512)     2097664   \n","                                                                 \n"," batch_normalization_26 (Bat  (None, 128, 128, 512)    2048      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 128, 128, 512)     0         \n","                                                                 \n"," conv2d_29 (Conv2D)          (None, 128, 128, 1)       8193      \n","                                                                 \n"," flatten (Flatten)           (None, 16384)             0         \n","                                                                 \n"," dense (Dense)               (None, 1024)              16778240  \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 1025      \n","                                                                 \n","=================================================================\n","Total params: 19,613,442\n","Trainable params: 19,611,522\n","Non-trainable params: 1,920\n","_________________________________________________________________\n","Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 256, 256, 3)]     0         \n","                                                                 \n"," Generator (Functional)      (None, 256, 256, 3)       11399171  \n","                                                                 \n"," Discriminator (Functional)  (None, 1)                 19613442  \n","                                                                 \n","=================================================================\n","Total params: 31,012,613\n","Trainable params: 31,000,197\n","Non-trainable params: 12,416\n","_________________________________________________________________\n"]}],"source":["from tensorflow import keras\n","from keras.layers import Input, Activation, Add, UpSampling2D, LeakyReLU, Conv2D\n","from keras.layers.core import Dense, Flatten, Lambda\n","from keras.layers import BatchNormalization\n","from keras.models import Model\n","\n","# The paper defined hyper-parameter: chr\n","channel_rate = 64\n","# Note the image_sape must be multiplied of patch_shape\n","image_shape = (256, 256, 3)\n","patch_shape = (channel_rate, channel_rate, 3)\n","\n","ngf = 64\n","ndf = 64\n","input_nc = 3\n","output_nc = 3\n","input_shape_generator = (256, 256, input_nc)\n","input_shape_discriminator = (256, 256, output_nc)\n","n_block_gen = 9\n","\n","\n","def generator_model():\n","    \"\"\"Build generator architecture..\"\"\"\n","\n","    inputs = Input(shape=image_shape)\n","\n","    x = ReflectionPadding2D((3, 3))(inputs)\n","    x = Conv2D(filters=ngf, kernel_size=(7, 7), padding='valid')(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    n_downsampling = 2\n","    for i in range(n_downsampling):\n","        mult = 2**i\n","        x = Conv2D(filters=ngf*mult*2, kernel_size=(3, 3),\n","                   strides=2, padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","    mult = 2**n_downsampling\n","    for i in range(n_block_gen):\n","        x = res_block(x, ngf*mult, use_dropout=True)\n","\n","    for i in range(n_downsampling):\n","        mult = 2**(n_downsampling - i)\n","        # x = Conv2DTranspose(filters=int(nfg*mult / 2), kernel_size=(3, 3), strides=2, padding='same')(x)\n","        x = UpSampling2D()(x)\n","        x = Conv2D(filters=int(ngf*mult / 2),\n","                   kernel_size=(3, 3), padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","\n","    x = ReflectionPadding2D((3, 3))(x)\n","    x = Conv2D(filters=output_nc, kernel_size=(7, 7), padding='valid')(x)\n","    x = Activation('tanh')(x)\n","\n","    outputs = Add()([x, inputs])\n","    outputs = keras.layers.Lambda(lambda z: z/2)(outputs)\n","\n","    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n","    return model\n","\n","\n","def discriminator_model():\n","    \"\"\"Build discriminator architecture.\"\"\"\n","\n","    n_layers, use_sigmoid = 3, False\n","    inputs = Input(shape=input_shape_discriminator)\n","\n","    x = Conv2D(filters=ndf, kernel_size=(4, 4),\n","               strides=2, padding='same')(inputs)\n","    x = LeakyReLU(0.2)(x)\n","\n","    nf_mult, nf_mult_prev = 1, 1\n","    for n in range(n_layers):\n","        nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n","        x = Conv2D(filters=ndf*nf_mult, kernel_size=(4, 4),\n","                   strides=1, padding='same')(x)\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU(0.2)(x)\n","\n","    nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n","    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4, 4),\n","               strides=1, padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(0.2)(x)\n","\n","    x = Conv2D(filters=1, kernel_size=(4, 4), strides=1, padding='same')(x)\n","    if use_sigmoid:\n","        x = Activation('sigmoid')(x)\n","\n","    x = Flatten()(x)\n","    x = Dense(1024, activation='tanh')(x)\n","    x = Dense(1, activation='sigmoid')(x)\n","\n","    model = Model(inputs=inputs, outputs=x, name='Discriminator')\n","    return model\n","\n","\n","def generator_containing_discriminator(generator, discriminator):\n","    inputs = Input(shape=image_shape)\n","    generated_image = generator(inputs)\n","    outputs = discriminator(generated_image)\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","\n","def generator_containing_discriminator_multiple_outputs(generator, discriminator):\n","    inputs = Input(shape=image_shape)\n","    generated_image = generator(inputs)\n","    outputs = discriminator(generated_image)\n","    model = Model(inputs=inputs, outputs=[generated_image, outputs])\n","    return model\n","\n","\n","if __name__ == '__main__':\n","    g = generator_model()\n","    g.summary()\n","    d = discriminator_model()\n","    d.summary()\n","    m = generator_containing_discriminator(\n","        generator_model(), discriminator_model())\n","    m.summary()\n"]},{"cell_type":"markdown","metadata":{},"source":["utils.py"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-15T17:12:16.931480Z","iopub.status.busy":"2023-07-15T17:12:16.930735Z","iopub.status.idle":"2023-07-15T17:12:16.946641Z","shell.execute_reply":"2023-07-15T17:12:16.945701Z","shell.execute_reply.started":"2023-07-15T17:12:16.931443Z"},"trusted":true},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","RESHAPE = (256, 256)\n","\n","\n","def is_an_image_file(filename):\n","    IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']\n","    for ext in IMAGE_EXTENSIONS:\n","        if ext in filename:\n","            return True\n","    return False\n","\n","\n","def list_image_files(directory):\n","    files = sorted(os.listdir(directory))\n","    return [os.path.join(directory, f) for f in files if is_an_image_file(f)]\n","\n","\n","def load_image(path):\n","    img = Image.open(path)\n","    return img\n","\n","\n","def preprocess_image(cv_img):\n","    cv_img = cv_img.resize(RESHAPE)\n","    img = np.array(cv_img)\n","    img = (img - 127.5) / 127.5\n","    return img\n","\n","\n","def deprocess_image(img):\n","    img = img * 127.5 + 127.5\n","    return img.astype('uint8')\n","\n","\n","def save_image(np_arr, path):\n","    img = np_arr * 127.5 + 127.5\n","    im = Image.fromarray(img)\n","    im.save(path)\n","\n","\n","def load_images(path, n_images):\n","    if n_images < 0:\n","        n_images = float(\"inf\")\n","    A_paths, B_paths = os.path.join(path, 'A'), os.path.join(path, 'B')\n","    all_A_paths, all_B_paths = list_image_files(\n","        A_paths), list_image_files(B_paths)\n","    images_A, images_B = [], []\n","    images_A_paths, images_B_paths = [], []\n","    for path_A, path_B in zip(all_A_paths, all_B_paths):\n","        img_A, img_B = load_image(path_A), load_image(path_B)\n","        images_A.append(preprocess_image(img_A))\n","        images_B.append(preprocess_image(img_B))\n","        images_A_paths.append(path_A)\n","        images_B_paths.append(path_B)\n","        if len(images_A) > n_images - 1:\n","            break\n","\n","    return {\n","        'A': np.array(images_A),\n","        'A_paths': np.array(images_A_paths),\n","        'B': np.array(images_B),\n","        'B_paths': np.array(images_B_paths)\n","    }\n","\n","\n","def write_log(callback, names, logs, batch_no):\n","    \"\"\"\n","    Util to write callback for Keras training\n","    \"\"\"\n","    for name, value in zip(names, logs):\n","        summary = tf.Summary()\n","        summary_value = summary.value.add()\n","        summary_value.simple_value = value\n","        summary_value.tag = name\n","        callback.writer.add_summary(summary, batch_no)\n","        callback.writer.flush()"]},{"cell_type":"raw","metadata":{},"source":["train.py"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-07-15T17:22:49.304225Z","iopub.status.busy":"2023-07-15T17:22:49.303559Z","iopub.status.idle":"2023-07-15T17:37:56.450129Z","shell.execute_reply":"2023-07-15T17:37:56.448372Z","shell.execute_reply.started":"2023-07-15T17:22:49.304186Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/4 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 490ms/step\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-15 17:23:49.031111: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_13/Generator/dropout_90/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"]},{"name":"stdout","output_type":"stream","text":["0.05560425270814449 11285.963889698187\n","1/1 [==============================] - 0s 31ms/step\n","0.02544535422930494 10581.75398670137\n","1/1 [==============================] - 0s 32ms/step\n","0.010465529724024236 9768.03116800083\n","1/1 [==============================] - 0s 34ms/step\n","-0.003207755711628124 9381.528065535129\n","1/1 [==============================] - 0s 33ms/step\n","-0.023478729627095164 8877.06560428357\n","1/1 [==============================] - 0s 32ms/step\n","-0.03970440891183292 8597.165293066171\n","1/1 [==============================] - 0s 32ms/step\n","-0.05010527684685907 8332.808982947157\n","1/1 [==============================] - 0s 27ms/step\n","-0.06113904242811259 7993.4636221789815\n","1/1 [==============================] - 0s 43ms/step\n","-0.0738611639658403 7740.56824566456\n","1/1 [==============================] - 0s 27ms/step\n","-0.09510312746046111 7519.48490844363\n","1/1 [==============================] - 0s 27ms/step\n","-0.10643078585536304 7359.623172093613\n","1/1 [==============================] - 0s 27ms/step\n","-0.1159576615532084 7166.3497580806015\n","1/1 [==============================] - 0s 39ms/step\n","-0.12714062891005037 7049.805988064376\n","1/1 [==============================] - 0s 31ms/step\n","-0.1431747828926226 6918.689027562212\n","1/1 [==============================] - 0s 36ms/step\n","-0.1664828271297544 6844.717099430998\n","1/1 [==============================] - 0s 55ms/step\n","-0.1794591992746973 6722.554587828778\n","1/1 [==============================] - 0s 28ms/step\n","-0.19621855783411507 6620.249527121443\n","1/1 [==============================] - 0s 27ms/step\n","-0.2101211730482141 6534.956038073952\n","1/1 [==============================] - 0s 27ms/step\n","-0.22238533212952652 6474.29579104\n","1/1 [==============================] - 0s 27ms/step\n","-0.2340855315481531 6403.115906281898\n","1/1 [==============================] - 0s 26ms/step\n","-0.24653335684931357 6327.086819614333\n","1/1 [==============================] - 0s 28ms/step\n","-0.25804755072415136 6265.347038047449\n","1/1 [==============================] - 0s 27ms/step\n","-0.2685668024160676 6211.232880530694\n","1/1 [==============================] - 0s 27ms/step\n","-0.2782088917547 6156.083072029575\n","1/1 [==============================] - 0s 27ms/step\n","-0.28700505717609476 6111.614239291851\n","1/1 [==============================] - 0s 27ms/step\n","-0.2951971703588282 6033.821620869048\n","1/1 [==============================] - 0s 26ms/step\n","-0.30010004213785846 5985.386650114611\n","1/1 [==============================] - 0s 27ms/step\n","-0.3066980311464807 5935.544893244285\n","1/1 [==============================] - 0s 29ms/step\n","-0.31277077203974535 5869.347991861503\n","1/1 [==============================] - 0s 27ms/step\n","-0.31901043672393087 5844.213896613233\n","1/1 [==============================] - 0s 32ms/step\n","-0.32458479653505384 5810.636013213902\n","1/1 [==============================] - 0s 53ms/step\n","-0.33002689729554024 5778.517401653883\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 1/4 [03:39<10:59, 219.98s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 28ms/step\n","-0.4993838360778525 4343.592691659927\n","1/1 [==============================] - 0s 27ms/step\n","-0.49968034065936495 4719.566342383623\n","1/1 [==============================] - 0s 29ms/step\n","-0.495446407220759 4526.476538274023\n","1/1 [==============================] - 0s 26ms/step\n","-0.444643337769867 4516.1503546436625\n","1/1 [==============================] - 0s 27ms/step\n","-0.4394699046432587 4326.198177564144\n","1/1 [==============================] - 0s 30ms/step\n","-0.43021103276748585 4310.402400066455\n","1/1 [==============================] - 0s 28ms/step\n","-0.40339522773827086 4423.610381410235\n","1/1 [==============================] - 0s 26ms/step\n","-0.410720974595851 4428.900220255057\n","1/1 [==============================] - 0s 30ms/step\n","-0.41857977848340966 4373.48269091519\n","1/1 [==============================] - 0s 26ms/step\n","-0.4265769973619815 4342.8862778743105\n","1/1 [==============================] - 0s 26ms/step\n","-0.4332516728382916 4334.20094502785\n","1/1 [==============================] - 0s 27ms/step\n","-0.4364141063980169 4298.7219472568895\n","1/1 [==============================] - 0s 26ms/step\n","-0.4411494737684211 4241.838406750025\n","1/1 [==============================] - 0s 27ms/step\n","-0.4453428167476316 4200.852021271984\n","1/1 [==============================] - 0s 27ms/step\n","-0.44866245384097037 4178.8351249423295\n","1/1 [==============================] - 0s 26ms/step\n","-0.4426361670406676 4172.742959317441\n","1/1 [==============================] - 0s 28ms/step\n","-0.4354447973379755 4154.106584750554\n","1/1 [==============================] - 0s 27ms/step\n","-0.4390128945967286 4158.133252187459\n","1/1 [==============================] - 0s 28ms/step\n","-0.4421833408396082 4126.846170471948\n","1/1 [==============================] - 0s 29ms/step\n","-0.4450741737976264 4097.856448297202\n","1/1 [==============================] - 0s 27ms/step\n","-0.44768968762808303 4069.0268145851674\n","1/1 [==============================] - 0s 29ms/step\n","-0.44978692964121975 4031.0811588831925\n","1/1 [==============================] - 0s 28ms/step\n","-0.45197010661334 4000.8294274025207\n","1/1 [==============================] - 0s 26ms/step\n","-0.4539713381794638 4007.3605999354686\n","1/1 [==============================] - 0s 27ms/step\n","-0.45581248464304186 3995.9929810655117\n","1/1 [==============================] - 0s 28ms/step\n","-0.4575120044644632 3980.150267439393\n","1/1 [==============================] - 0s 36ms/step\n","-0.45908563214166626 3962.055168759308\n","1/1 [==============================] - 0s 27ms/step\n","-0.46054685956517816 3947.490972879032\n","1/1 [==============================] - 0s 36ms/step\n","-0.4619073126827172 3920.7191800971827\n","1/1 [==============================] - 0s 27ms/step\n","-0.46317706892662663 3928.638733017776\n","1/1 [==============================] - 0s 27ms/step\n","-0.46436490541283576 3921.277119324092\n","1/1 [==============================] - 0s 28ms/step\n","-0.46547850211868075 3902.293866354041\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 2/4 [07:11<07:09, 214.94s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 27ms/step\n","-0.49999988079054614 3901.4308990240097\n","1/1 [==============================] - 0s 28ms/step\n","-0.499999909876833 3798.367053886255\n","1/1 [==============================] - 0s 28ms/step\n","-0.49999993991779246 3562.2982198993363\n","1/1 [==============================] - 0s 26ms/step\n","-0.49999991768544094 3616.8577636828027\n","1/1 [==============================] - 0s 28ms/step\n","-0.4894552236163684 3627.6902217904726\n","1/1 [==============================] - 0s 28ms/step\n","-0.4768678785305721 3607.493870112631\n","1/1 [==============================] - 0s 26ms/step\n","-0.4777159062980439 3529.6170856867516\n","1/1 [==============================] - 0s 28ms/step\n","-0.4804625267251274 3589.0509421105185\n","1/1 [==============================] - 0s 28ms/step\n","-0.4826333570881462 3560.0145523349443\n","1/1 [==============================] - 0s 27ms/step\n","-0.48376357034220385 3559.1253625154495\n","1/1 [==============================] - 0s 27ms/step\n","-0.4846201697171237 3499.5012121724358\n","1/1 [==============================] - 0s 29ms/step\n","-0.48587428596163884 3479.9229687667557\n","1/1 [==============================] - 0s 27ms/step\n","-0.48579612505854347 3500.961871099778\n","1/1 [==============================] - 0s 27ms/step\n","-0.48673484778417403 3479.421120852232\n","1/1 [==============================] - 0s 28ms/step\n","-0.4786643403489747 3454.7143368734255\n","1/1 [==============================] - 0s 27ms/step\n","-0.47463513834333054 3448.587714511901\n","1/1 [==============================] - 0s 27ms/step\n","-0.47501558488212964 3450.0636173755515\n","1/1 [==============================] - 0s 27ms/step\n","-0.47638697526810897 3418.2168480036435\n","1/1 [==============================] - 0s 30ms/step\n","-0.47758524135987235 3400.7397253837503\n","1/1 [==============================] - 0s 29ms/step\n","-0.47870529412784263 3378.3100599755844\n","1/1 [==============================] - 0s 27ms/step\n","-0.47971146446869006 3370.7053419682716\n","1/1 [==============================] - 0s 27ms/step\n","-0.4806336602071738 3368.689375774427\n","1/1 [==============================] - 0s 29ms/step\n","-0.48147567493821447 3350.987436157206\n","1/1 [==============================] - 0s 27ms/step\n","-0.4822475158533429 3339.031230319705\n","1/1 [==============================] - 0s 27ms/step\n","-0.4829180095098282 3332.1374228954314\n","1/1 [==============================] - 0s 30ms/step\n","-0.48325801720048306 3319.471401456839\n","1/1 [==============================] - 0s 28ms/step\n","-0.4838780787973461 3301.0499188451117\n","1/1 [==============================] - 0s 38ms/step\n","-0.48445386168746846 3311.423058357267\n","1/1 [==============================] - 0s 26ms/step\n","-0.48498993424651976 3302.082268556644\n","1/1 [==============================] - 0s 26ms/step\n","-0.48548408854974145 3290.265935680601\n","1/1 [==============================] - 0s 27ms/step\n","-0.4859453361733635 3293.8701401705384\n","1/1 [==============================] - 0s 27ms/step\n","-0.4863845397611911 3284.153935468445\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 3/4 [10:44<03:34, 214.23s/it]"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 27ms/step\n","-0.49869652395725267 3186.0610376993814\n","1/1 [==============================] - 0s 26ms/step\n","-0.4991906762385329 2811.7962443331876\n","1/1 [==============================] - 0s 27ms/step\n","-0.49945940178393927 2992.5558526979553\n","1/1 [==============================] - 0s 28ms/step\n","-0.4995945513156199 2934.248821551601\n","1/1 [==============================] - 0s 27ms/step\n","-0.4996756410327152 2887.9500387271246\n","1/1 [==============================] - 0s 27ms/step\n","-0.49943407076320223 2880.56840634346\n","1/1 [==============================] - 0s 27ms/step\n","-0.4995149177942201 2862.362172115417\n","1/1 [==============================] - 0s 26ms/step\n","-0.49957516869701235 2846.9195943921804\n","1/1 [==============================] - 0s 29ms/step\n","-0.4996223721748705 2855.187799482434\n","1/1 [==============================] - 0s 27ms/step\n","-0.49965316434238816 2857.293266113599\n","1/1 [==============================] - 0s 27ms/step\n","-0.49968468760311496 2895.875424823978\n","1/1 [==============================] - 0s 27ms/step\n","-0.4997109636360283 2885.7747824672197\n","1/1 [==============================] - 0s 27ms/step\n","-0.49973319691249607 2882.346745727918\n","1/1 [==============================] - 0s 27ms/step\n","-0.49975225425450104 2890.15880371985\n","1/1 [==============================] - 0s 28ms/step\n","-0.4997687706375337 2889.477030506399\n","1/1 [==============================] - 0s 28ms/step\n","-0.49978309059741094 2875.4937493490675\n","1/1 [==============================] - 0s 27ms/step\n","-0.4997958470443788 2874.9900612293504\n","1/1 [==============================] - 0s 28ms/step\n","-0.4998071888752373 2856.3016500230187\n","1/1 [==============================] - 0s 27ms/step\n","-0.4998172538259979 2904.7388608016468\n","1/1 [==============================] - 0s 27ms/step\n","-0.4998263911152342 2913.8144890348117\n","1/1 [==============================] - 0s 26ms/step\n","-0.4998346582049842 2911.879157457087\n","1/1 [==============================] - 0s 30ms/step\n","-0.49984217323556934 2915.95708012039\n","1/1 [==============================] - 0s 27ms/step\n","-0.49984903526880514 2917.509099630342\n","1/1 [==============================] - 0s 27ms/step\n","-0.4998548707337002 2924.0641482869783\n","1/1 [==============================] - 0s 26ms/step\n","-0.499860675904352 2931.5517059278486\n","1/1 [==============================] - 0s 26ms/step\n","-0.4998644969845021 2944.8462138558048\n","1/1 [==============================] - 0s 30ms/step\n","-0.499869513251149 2934.5946811018166\n","1/1 [==============================] - 0s 28ms/step\n","-0.4998737268829028 2941.2191598167024\n","1/1 [==============================] - 0s 27ms/step\n","-0.49987808112572474 2957.332551133701\n","1/1 [==============================] - 0s 26ms/step\n","-0.4998821450879846 2970.0790061304965\n","1/1 [==============================] - 0s 28ms/step\n","-0.49988594682225784 2968.7539730363637\n","1/1 [==============================] - 0s 27ms/step\n","-0.4998895109582426 2959.031850552497\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4/4 [14:16<00:00, 214.20s/it]\n"]}],"source":["import os\n","import datetime\n","import numpy as np\n","import tqdm\n","from keras.callbacks import TensorBoard\n","from keras.optimizers import Adam\n","\n","BASE_DIR = '/kaggle/working/training/'\n","\n","\n","def save_all_weights(d, g, epoch_number, current_loss):\n","    now = datetime.datetime.now()\n","    save_dir = os.path.join(BASE_DIR, '{}{}'.format(now.month, now.day))\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","    g.save_weights(os.path.join(save_dir, 'generator_{}_{}.h5'.format(\n","        epoch_number, current_loss)), True)\n","    d.save_weights(os.path.join(\n","        save_dir, 'discriminator_{}.h5'.format(epoch_number)), True)\n","\n","\n","def train_multiple_outputs(n_images, batch_size, log_dir, epoch_num, critic_updates=5):\n","    data = load_images('/kaggle/working/train', n_images)\n","    y_train, x_train = data['B'], data['A']\n","\n","    g = generator_model()\n","    d = discriminator_model()\n","    d_on_g = generator_containing_discriminator_multiple_outputs(g, d)\n","\n","    d_opt = Adam(learning_rate=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    d_on_g_opt = Adam(learning_rate=1E-4, beta_1=0.9,\n","                      beta_2=0.999, epsilon=1e-08)\n","\n","    d.trainable = True\n","    d.compile(optimizer=d_opt, loss=wasserstein_loss)\n","    d.trainable = False\n","    loss = [perceptual_loss, wasserstein_loss]\n","    loss_weights = [100, 1]\n","    d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n","    d.trainable = True\n","    \n","    output_true_batch, output_false_batch = np.ones(\n","        (batch_size, 1)), -np.ones((batch_size, 1))\n","\n","    log_path = 'kaggle/working/logs'\n","    tensorboard_callbacks = TensorBoard(log_path)\n","\n","    for epoch in tqdm.tqdm(range(epoch_num)):\n","        permutated_indexes = np.random.permutation(x_train.shape[0])\n","\n","        d_losses = []\n","        d_on_g_losses = []\n","        for index in range(int(x_train.shape[0] / batch_size)):\n","            batch_indexes = permutated_indexes[index *\n","                                               batch_size:(index+1)*batch_size]\n","            image_blur_batch = x_train[batch_indexes]\n","            image_full_batch = y_train[batch_indexes]\n","\n","            generated_images = g.predict(\n","                x=image_blur_batch, batch_size=batch_size)\n","\n","            for _ in range(critic_updates):\n","                d_loss_real = d.train_on_batch(\n","                    image_blur_batch, output_true_batch)\n","                d_loss_fake = d.train_on_batch(\n","                    generated_images, output_false_batch)\n","                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n","                d_losses.append(d_loss)\n","\n","            d.trainable = False\n","\n","            d_on_g_loss = d_on_g.train_on_batch(\n","                image_blur_batch, [image_full_batch, output_true_batch])\n","            d_on_g_losses.append(d_on_g_loss)\n","\n","            d.trainable = True\n","\n","            print(np.mean(d_losses), np.mean(d_on_g_losses))\n","            with open('long.txt', 'a+') as f:\n","                f.write('{} - {}\\n'.format(epoch,\n","                        np.mean(d_losses), np.mean(d_on_g_losses)))\n","\n","            save_all_weights(d, g, epoch, int(np.mean(d_on_g_losses)))\n","\n","\n","def train_command(n_images, batch_size, log_dir, epoch_num, critic_updates):\n","    return train_multiple_outputs(n_images, batch_size, log_dir, epoch_num, critic_updates)\n","\n","\n","if __name__ == '__main__':\n","    train_command(512, 16, False, 4, 5)"]},{"cell_type":"markdown","metadata":{},"source":["test.py"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-07-15T17:50:29.350183Z","iopub.status.busy":"2023-07-15T17:50:29.349800Z","iopub.status.idle":"2023-07-15T17:50:32.967560Z","shell.execute_reply":"2023-07-15T17:50:32.966490Z","shell.execute_reply.started":"2023-07-15T17:50:29.350153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n"]}],"source":["import numpy as np\n","from PIL import Image\n","\n","def test(batch_size):\n","    data = load_images('/kaggle/working/test', batch_size)\n","    y_test, x_test = data['B'], data['A']\n","    g = generator_model()\n","    g.load_weights('/kaggle/working/kaggle/working/training/715/generator_3_2811.h5')\n","    generator_images = g.predict(x=x_test, batch_size=batch_size)\n","    generated = np.array([deprocess_image(img) for img in generator_images])\n","\n","    x_test = deprocess_image(x_test)\n","    y_test = deprocess_image(y_test)\n","\n","    for i in range(generator_images.shape[0]):\n","        y = y_test[i, :, :, :]\n","        x = x_test[i, :, :, :]\n","        img = generated[i, :, :, :]\n","        output = np.concatenate((y, x, img), axis=1)\n","        im = Image.fromarray(output.astype(np.uint8))\n","        im.save('results{}.png'.format(i))\n","\n","\n","def test_command(batch_size):\n","    return test(batch_size)\n","\n","\n","if __name__ == '__main__':\n","    test_command(4)"]},{"cell_type":"markdown","metadata":{},"source":["deblur_image.py"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 872ms/step\n","1/1 [==============================] - 1s 592ms/step\n"]}],"source":["import numpy as np\n","from PIL import Image\n","import os\n","\n","\n","def deblur(weight_path, input_dir, output_dir):\n","    g = generator_model()\n","    g.load_weights(weight_path)\n","    for image_name in os.listdir(input_dir):\n","        image = np.array(\n","            [preprocess_image(load_image(os.path.join(input_dir, image_name)))])\n","        x_test = image\n","        generated_images = g.predict(x=x_test)\n","        generated = np.array([deprocess_image(img)\n","                             for img in generated_images])\n","        x_test = deprocess_image(x_test)\n","        for i in range(generated_images.shape[0]):\n","            x = x_test[i, :, :, :]\n","            img = generated[i, :, :, :]\n","            output = np.concatenate((x, img), axis=1)\n","            im = Image.fromarray(output.astype(np.uint8))\n","            im.save(os.path.join(output_dir, image_name))\n","\n","\n","def deblur_command(weight_path, input_dir, output_dir):\n","    return deblur(weight_path, input_dir, output_dir)\n","\n","\n","if __name__ == \"__main__\":\n","    deblur_command(weight_path='./generator_3_2811.h5',\n","                   input_dir='../demo/input', output_dir='../demo/output/')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
